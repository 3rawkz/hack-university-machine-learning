{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Single layer Percetrons to Multi-layer neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline\n",
    "* Multi-layer structure\n",
    "* Activation functions\n",
    "* Back-Propagation Algorithm\n",
    "* Stepping through Theano, Lasagne and NoLearn\n",
    "* Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-layer structure\n",
    "![Multi-layer neural network](http://ufldl.stanford.edu/tutorial/images/Network3322.png)\n",
    "Source: http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "* Choose an activation function which has a simple derivative\n",
    "\n",
    "## Sigmoid function\n",
    "\n",
    "$$ f(x) = \\frac{1}{1+e^{-x}} $$\n",
    "\n",
    "$$ f'(x) = f(x) (1-f(x)) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "f = 1/(1+np.exp(-(x))) # +0.1 to avoid dividing by 0\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.gca()\n",
    "ax.grid()\n",
    "plt.plot(x, f, color='black')\n",
    "plt.xlim(x.min(), x.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Hyperbolic tangent function\n",
    "\n",
    "$$ f(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$\n",
    "\n",
    "$$ f(x) = 2 * \\frac{1}{1+e^{-2x}} - 1$$\n",
    "\n",
    "$$ f'(x) = 1 - f(x)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "f = 2/(1+np.exp(-(2*x)))-1 # +0.1 to avoid dividing by 0\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.gca()\n",
    "ax.grid()\n",
    "plt.plot(x, f, color='black')\n",
    "plt.xlim(x.min(), x.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* [Great comparison of activation functions](https://en.wikipedia.org/wiki/Activation_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation Algorithm\n",
    "\n",
    "1. Compute forward pass by calculating the activations for $a^{(2)}$ and $a^{(3)}$\n",
    "2. Calculate the cost function $$ J(w) = \\frac{1}{2}(a^{(3)} - y)^2$$\n",
    "3. Calculate the errors $$ \\delta^{(3)} = a^{(3)} - y $$\n",
    "4. Calculate the error for the hidden layer $$ \\delta^{(2)} = (W^{(2)})^T \\delta^{(3)} * \\frac{\\delta \\phi (z^{(2)})}{\\delta z^{(2)}}$$\n",
    "Interestingly, $$ \\frac{\\delta \\phi (z^{(2)})}{\\delta z^{(2)}} = (a^{(2)} \\dot (1-a^{(2)}))$$ \n",
    "\n",
    "5. Calculate the change of the weights\n",
    "$$ \\Delta^{(l)}_{i,j} = \\Delta^{(l)}_{i,j} + a^{(l)}_{j} \\delta^{(l+1)}_{i}$$\n",
    "6. Update the weights\n",
    "$$ W^{(l)} = W^{(l)} - \\eta \\Delta^{(l)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools to calculate neural networks\n",
    "\n",
    " * Theano: Symbolic computation library for Python (Cuda support)\n",
    " * Lasagne: Neural network library based on Theano\n",
    " * NoLearn: Python wrapper for Lasagne\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NoLearn\n",
    "In NoLearn, you can define the network layers as a Python list\n",
    "\n",
    "```\n",
    "layers = [\n",
    "    (InputLayer, {'shape': (1, X.shape[1],)}),\n",
    "    (DenseLayer, {'num_units': 2, 'nonlinearity': sigmoid}),\n",
    "    (DenseLayer, {'num_units': 2, 'nonlinearity': softmax}),\n",
    "]\n",
    "```\n",
    "\n",
    "and pass it to the neural network definition\n",
    "```\n",
    "net1 = NeuralNet(\n",
    "    layers=layers,\n",
    "    max_epochs=100,\n",
    "    update_learning_rate=1,\n",
    "    train_split=TrainSplit(eval_size=0),\n",
    "    verbose=3,\n",
    ")\n",
    "```\n",
    "\n",
    "Use `net.fit(X, y)` and `net.predict(X)` to train and for your prediction, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Things to do:\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XOR prediction with a Multi-layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.nonlinearities import softmax, sigmoid\n",
    "\n",
    "from nolearn.lasagne import TrainSplit\n",
    "from nolearn.lasagne import NeuralNet\n",
    "\n",
    "data_set = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0],\n",
    "])\n",
    "\n",
    "X = data_set[:, :2]\n",
    "y = data_set[:, 2:]\n",
    "X = np.array(X).astype(np.float32)\n",
    "y = np.array(y).ravel().astype(np.int32)\n",
    "\n",
    "layers = [\n",
    "    (InputLayer, {'shape': (1, X.shape[1],)}),\n",
    "    # add hidden and output layers\n",
    "    # ...\n",
    "]\n",
    "\n",
    "net1 = NeuralNet(\n",
    "    layers=layers,\n",
    "    # setup network training parameters\n",
    "    # ...\n",
    ")\n",
    "net1.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test the model by predicting the output for (1, 1)\n",
    "net1.predict([[1, 1],])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit card approval prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data from https://onlinecourses.science.psu.edu/stat857/node/215\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.nonlinearities import softmax, tanh\n",
    "\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from nolearn.lasagne import TrainSplit\n",
    "\n",
    "training_set = pd.read_csv('../../data/German_credit_card_training_500.csv')\n",
    "training_set = training_set.sort_values(\n",
    "    ['Creditability',]).head(\n",
    "    2 * len(training_set[(training_set['Creditability'] == 0)])\n",
    ")\n",
    "# test_set = pd.read_csv('german_credit_dataset/Test50.csv')\n",
    "# extract the creditability column as y vector\n",
    "y = training_set['Creditability'].values\n",
    "# drop the creditability column from the dataset\n",
    "training_set.drop('Creditability', axis=1, inplace=True)\n",
    "\n",
    "# remaining dataset is used as input matrix\n",
    "X = np.array(training_set.values).astype(np.float32)\n",
    "y = np.array(y).astype(np.int32)\n",
    "\n",
    "# apply some very simple normalization to the data\n",
    "X -= X.mean()\n",
    "X /= X.std()\n",
    "\n",
    "credit_approval_net = NeuralNet(\n",
    "    layers=[  # three layers: one hidden layer\n",
    "        (InputLayer, {'shape': (None, X.shape[1],)}),\n",
    "        # add hidden and output layers\n",
    "        # ...\n",
    "        ],\n",
    "    # setup the training parameters\n",
    "    )\n",
    "\n",
    "credit_approval_net.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# determine score on test set\n",
    "training_set = pd.read_csv('../../data/German_credit_card_test_500.csv')\n",
    "training_set = training_set.sort_values(\n",
    "    ['Creditability',]).head(\n",
    "    2 * len(training_set[(training_set['Creditability'] == 0)])\n",
    ")\n",
    "# test_set = pd.read_csv('german_credit_dataset/Test50.csv')\n",
    "# extract the creditability column as y vector\n",
    "y_test = training_set['Creditability'].values\n",
    "# drop the creditability column from the dataset\n",
    "training_set.drop('Creditability', axis=1, inplace=True)\n",
    "\n",
    "# remaining dataset is used as input matrix\n",
    "X_test = np.array(training_set.values).astype(np.float32)\n",
    "y_test = np.array(y_test).astype(np.int32)\n",
    "\n",
    "# apply some very simple normalization to the data\n",
    "X_test -= X_test.mean()\n",
    "X_test /= X_test.std()\n",
    "\n",
    "# run test set with test data\n",
    "credit_approval_net.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify hand-written numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle, gzip, numpy\n",
    "\n",
    "# Load the dataset\n",
    "f = gzip.open('../../data/mnist.pkl.gz', 'rb')\n",
    "training_set, valid_set, test_set = cPickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = training_set[0]\n",
    "y_train = training_set[1]\n",
    "X_test = test_set[0]\n",
    "y_test = test_set[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "\n",
    "def get_images(training_set):\n",
    "    \"\"\" Return a list containing the images from the MNIST data\n",
    "    set. Each image is represented as a 2-d numpy array.\n",
    "    \n",
    "    source: https://github.com/mnielsen/neural-networks-and-deep-learning/blob/master/fig/mnist.py\n",
    "    \"\"\"\n",
    "    flattened_images = training_set[0]\n",
    "    return [np.reshape(f, (-1, 28)) for f in flattened_images]\n",
    "\n",
    "def plot_10_by_10_images(images):\n",
    "    \"\"\" Plot 100 MNIST images in a 10 by 10 table. \"\"\"\n",
    "    figs, axes = plt.subplots(4, 4, figsize=(6, 6))\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            axes[i, j].imshow(-X_train[i + 4 * j].reshape(28, 28), cmap='gray', interpolation='none')\n",
    "            axes[i, j].set_xticks([])\n",
    "            axes[i, j].set_yticks([])\n",
    "            axes[i, j].set_title(\"Label: {}\".format(y[i + 4 * j]))\n",
    "            axes[i, j].axis('off')\n",
    "    \n",
    "images = get_images(training_set)\n",
    "plot_10_by_10_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lasagne.layers import DenseLayer\n",
    "from lasagne.layers import InputLayer\n",
    "from lasagne.nonlinearities import softmax, tanh\n",
    "\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from nolearn.lasagne import TrainSplit\n",
    "\n",
    "X = X_train.astype(np.float32)\n",
    "y = y_train.astype(np.int32)\n",
    "\n",
    "# apply some very simple normalization to the data\n",
    "X -= X.mean()\n",
    "X /= X.std()\n",
    "\n",
    "mnist_net = NeuralNet(\n",
    "    layers=[  # three layers: one hidden layer\n",
    "        (InputLayer, {'shape': (None, X.shape[1], )}),\n",
    "        (DenseLayer, {'num_units': 50, 'nonlinearity': sigmoid}),\n",
    "        (DenseLayer, {'num_units': 10, 'nonlinearity': softmax}),\n",
    "        ],\n",
    "    update_learning_rate=0.1,\n",
    "    max_epochs=5,  # we want to train this many epochs\n",
    "    verbose=2,\n",
    "    train_split=TrainSplit(eval_size=0.25),\n",
    "    )\n",
    "\n",
    "mnist_net.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mnist_net.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Effective Backpropagation, pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n",
    "* [Comparison of activation functions](https://en.wikipedia.org/wiki/Activation_function)\n",
    "* [Introduction to Theano](http://on-demand.gputechconf.com/gtc/2015/webinar/deep-learning-course/getting-started-with-theano.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
